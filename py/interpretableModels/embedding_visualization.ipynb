{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2y4nDg2SdkbG"
   },
   "source": [
    "# XAI CODE DEMO\n",
    "\n",
    "## Explainable AI Specialization on Coursera"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NXwugtsydozs"
   },
   "source": [
    "# Visualizing Embedding (Latent) Space üîé\n",
    "\n",
    "If you experience high latency while running this notebook, you can open it in Google Colab:\n",
    "\n",
    "[![Open In Collab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/explainable-machine-learning/explainable-ml/blob/main/embedding_visualization.ipynb)\n",
    "\n",
    "* How do we go about visualizing the latent space?\n",
    "* With so many dimensions, can we make any meaningful interpretations of the latent space?\n",
    "\n",
    "#### Dimensionality Reduction & Visualization:\n",
    "* PCA\n",
    "* t-SNE\n",
    "* UMAP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iWAi_aAXWyfU",
    "outputId": "103fcaf6-9467-478f-880a-c955ae6e401d"
   },
   "outputs": [],
   "source": [
    "!pip install gensim==4.3.2 matplotlib==3.7.1 scikit-learn==1.2.2 umap-learn==0.5.6 plotly==5.15.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "alrzhEnoW6x6"
   },
   "outputs": [],
   "source": [
    "# Basic\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "\n",
    "# Dimensionality Reduction\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import umap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aaw8t4WzZy17"
   },
   "source": [
    "For this demo, we will be using **GloVe embeddings**, which is a traditional NLP embedding model. This particular version has a vector length of only 50 (reminder that Open AI‚Äôs ada_002 embedding model has a vector length of 1536!)\n",
    "\n",
    "**GloVe** is an unsupervised learning algorithm for obtaining vector representations for words. Training is performed on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations showcase interesting linear substructures of the word vector space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______________\n",
    "The code block below can be run outside of the Coursera environment to get the pretrained GloVe model and embed words. We have done this for you, so instead you will load the words and embeddings into this notebook."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sdgLJfLHW9--",
    "outputId": "5828862e-73aa-48a7-9b18-2ecf20792057"
   },
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "# Load pre-trained GloVe model (50-dimensional vectors)\n",
    "model_name = 'glove-wiki-gigaword-50'\n",
    "glove_model = api.load(model_name)\n",
    "\n",
    "# Select a subset of words for visualization\n",
    "subset_size = 500\n",
    "words = list(glove_model.key_to_index)[:subset_size]\n",
    "embeddings = np.array([glove_model[word] for word in words])\n",
    "\n",
    "print(f\"Embeddings shape: {embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_words_and_embeddings(words_file='words.npy', embeddings_file='embeddings.npy'):\n",
    "    # Load words\n",
    "    loaded_words = np.load(words_file, allow_pickle=True)\n",
    "    \n",
    "    # Load embeddings\n",
    "    loaded_embeddings = np.load(embeddings_file)\n",
    "    \n",
    "    print(f\"Loaded words: {len(loaded_words)}\")\n",
    "    print(f\"Loaded embeddings shape: {loaded_embeddings.shape}\")\n",
    "    \n",
    "    return loaded_words, loaded_embeddings\n",
    "\n",
    "words, embeddings = load_words_and_embeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ij7kiPoEazfa"
   },
   "source": [
    "## Principal Components Analysis (PCA)\n",
    "\n",
    "* Focus is on capturing global linear relationships in the data\n",
    "* Use to: simplify and find global linear relationships and patterns in the data\n",
    "\n",
    "#### How does PCA work?\n",
    "\n",
    "1. Standardize the Data: Scale the data so each feature has a mean of 0 and standard deviation of 1\n",
    "2. Compute the Covariance Matrix: Calculate the covariance matrix to understand how features vary together\n",
    "3. Compute Eigenvalues and Eigenvectors: Derive the eigenvalues and eigenvectors from the covariance matrix. Eigenvectors represent principal components, and eigenvalues indicate their significance\n",
    "4. Sort Eigenvalues and Eigenvectors: Order them by descending eigenvalues to prioritize the most significant components\n",
    "5. Select Principal Components: Choose the top ùëò eigenvectors corresponding to the largest eigenvalues\n",
    "6. Transform the Data: Project the original data onto the selected principal components to reduce dimensions\n",
    "\n",
    "#### Implementation in Python\n",
    "Need to set:\n",
    "* `n_components` - The number of dimensions in the embedded space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "jv95-yoHYz72",
    "outputId": "b04aa449-ba6c-4482-9905-51aac5c6b079"
   },
   "outputs": [],
   "source": [
    "# Apply PCA\n",
    "pca = PCA(n_components=2)\n",
    "embeddings_pca = pca.fit_transform(embeddings)\n",
    "\n",
    "# Plot PCA results using Plotly for interactivity\n",
    "fig_pca = px.scatter(\n",
    "    embeddings_pca, x=0, y=1,\n",
    "    text=words,\n",
    "    title=\"PCA of GloVe Embeddings\",\n",
    "    labels={'0': 'Principal Component 1', '1': 'Principal Component 2'}\n",
    ")\n",
    "fig_pca.update_traces(marker=dict(size=8))\n",
    "fig_pca.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Er94NLcObojs"
   },
   "source": [
    "## t-distributed Stochastic Neighbor Embedding (t-SNE)\n",
    "\n",
    "* Constructs a lower-dimensional representation where similar data points are placed closer together\n",
    "* Use to: Emphasize visualization, reveal local patterns and clusters\n",
    "\n",
    "\n",
    "#### How does t-SNE work?\n",
    "\n",
    "1. Compute Pairwise Similarities: Measure how similar each pair of data points is in the high-dimensional space using a Gaussian kernel\n",
    "2. Initialize Embeddings: Start with random low-dimensional embeddings for each data point\n",
    "3. Compute Similarities in Low-Dimensional Space: Measure similarities between low-dimensional embeddings using a Student's t-distribution\n",
    "4. Optimize Embeddings: Adjust the embeddings to minimize the difference between the distributions of similarities in high-dimensional and low-dimensional spaces\n",
    "5. Reduce Dimensionality: Obtain a reduced-dimensional representation of the data, preserving local relationships between data points\n",
    "\n",
    "#### Implementation in Python\n",
    "Need to set:\n",
    "* `n_components` - The number of dimensions in the embedded space\n",
    "* `perplexity` - a hyperparameter that balances the attention given to local versus global aspects of the data. It affects the quality of the resulting embeddings. Higher perplexity values consider more points as neighbors of each other, potentially resulting in more global views of the data.\n",
    "* `n_iter` - the number of iterations the algorithm will run for. More iterations can lead to better convergence and potentially better embeddings, but it also increases computation time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "9MM7MaNNZEbI",
    "outputId": "37b52782-4b6d-4a13-ae3e-de35159c9e6a"
   },
   "outputs": [],
   "source": [
    "# Apply t-SNE\n",
    "tsne = TSNE(n_components=2, perplexity=30, n_iter=300, random_state=42)\n",
    "embeddings_tsne = tsne.fit_transform(embeddings)\n",
    "\n",
    "# Plot t-SNE results using Plotly\n",
    "fig_tsne = px.scatter(\n",
    "    embeddings_tsne, x=0, y=1,\n",
    "    text=words,\n",
    "    title=\"t-SNE of GloVe Embeddings\",\n",
    "    labels={'0': 'Component 1', '1': 'Component 2'}\n",
    ")\n",
    "fig_tsne.update_traces(marker=dict(size=8))\n",
    "fig_tsne.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TLyGrPrXctbR"
   },
   "source": [
    "## Uniform Manifold Approximation and Projection (UMAP)\n",
    "\n",
    "* Uses manifold learning (nonlinear dimensionality reduction) to understand the underlying structure or shape of the data\n",
    "* Focus on capturing complex, non-linear relationships in the data\n",
    "* Use to: preserve local structure and handle complex, nonlinear relationships\n",
    "\n",
    "\n",
    "\n",
    "#### How does UMAP work?\n",
    "1. Construct Local Neighborhoods: Define local neighborhoods for each data point in the high-dimensional space based on proximity\n",
    "2. Optimize Low-Dimensional Embeddings: Minimize the discrepancy between local neighborhoods in the high-dimensional and low-dimensional spaces using stochastic gradient descent\n",
    "3. Preserve Global Structure: Balance the preservation of local and global structures using a fuzzy simplicial set representation\n",
    "4. Reduce Dimensionality: Obtain a lower-dimensional representation of the data while preserving both local and global relationships\n",
    "5. Effective Visualization: UMAP provides an effective tool for visualizing high-dimensional data in a reduced-dimensional space, capturing complex relationships and structures\n",
    "\n",
    "\n",
    "#### Implementation in Python\n",
    "Need to set:\n",
    "* `n_components` - The number of dimensions in the embedded space\n",
    "* `n_neighbors` - determines the number of neighboring points used in the construction of the high-dimensional fuzzy topological representation of the data. It controls the local connectivity structure in the high-dimensional space. Higher values result in a more global view of the data, while lower values emphasize local structure\n",
    "* `min_dist` - controls the minimum distance between embedded points in the low-dimensional representation. It acts as a regularization parameter preventing points from being too close to each other in the embedding space. Larger values of min_dist result in a more spread-out embedding, while smaller values allow points to be closer together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 613
    },
    "id": "IKOsaYOSZRhQ",
    "outputId": "98fb010e-afd8-4fc5-8491-e27fcd407129"
   },
   "outputs": [],
   "source": [
    "# This line hides a low-level depracation warning. This warning is within the UMAP library itself.\n",
    "os.environ['KMP_WARNINGS'] = '0'\n",
    "\n",
    "# Apply UMAP\n",
    "umap_model = umap.UMAP(n_components=2, n_neighbors=15, min_dist=0.1, random_state=42)\n",
    "embeddings_umap = umap_model.fit_transform(embeddings)\n",
    "\n",
    "# Plot UMAP results using Plotly\n",
    "fig_umap = px.scatter(\n",
    "    embeddings_umap, x=0, y=1,\n",
    "    text=words,\n",
    "    title=\"UMAP of GloVe Embeddings\",\n",
    "    labels={'0': 'Component 1', '1': 'Component 2'}\n",
    ")\n",
    "fig_umap.update_traces(marker=dict(size=8))\n",
    "fig_umap.show()\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
