{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nSpVBPe6W978"
   },
   "source": [
    "# XAI CODE DEMO\n",
    "\n",
    "## Explainable AI Specialization on Coursera"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eIWpbUcEWnVA"
   },
   "source": [
    "# Global Explanations 🌎\n",
    "\n",
    "\n",
    "If you experience high latency when running this notebook, you can Open in Google Colab:\n",
    "[![Open In Collab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/explainable-machine-learning/explainable-ml/blob/main/global_explanations.ipynb)\n",
    "\n",
    "Table of Contents\n",
    "* [Partial Dependence Plots](https://colab.research.google.com/drive/1WhBODDgVGE4NdnWtCouRuFn8rj0oxT6K#scrollTo=b9WMrsxrSc4Z&line=1&uniqifier=1)\n",
    "* [ALE Plots](https://colab.research.google.com/drive/1WhBODDgVGE4NdnWtCouRuFn8rj0oxT6K#scrollTo=H4na5VBGOath&line=6&uniqifier=1)\n",
    "* [Permutation Feature Importance](https://colab.research.google.com/drive/1WhBODDgVGE4NdnWtCouRuFn8rj0oxT6K#scrollTo=SLIGY1TLbSVm&line=21&uniqifier=1)\n",
    "* [Feature Interaction](https://colab.research.google.com/drive/1WhBODDgVGE4NdnWtCouRuFn8rj0oxT6K#scrollTo=Fn1TE2O-aByw&line=6&uniqifier=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C7Cd8ixTI-Xj",
    "outputId": "f790682d-efbc-43cd-be6b-6e22929fa8d5"
   },
   "outputs": [],
   "source": [
    "!pip install numpy==1.25.2 pandas==2.0.3 scikit-learn==1.2.2 shap==0.45.1 xgboost==1.7.5\n",
    "!pip install git+https://github.com/MaximeJumelle/ALEPython.git@dev#egg=alepython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DUzwROK5JDaS"
   },
   "outputs": [],
   "source": [
    "# Data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Models\n",
    "import xgboost\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# XAI\n",
    "import shap\n",
    "from alepython import ale_plot\n",
    "from sklearn.inspection import PartialDependenceDisplay\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iaaVxMhCSXoS"
   },
   "source": [
    "**Dataset**\n",
    "We will use the classic [UCI adult income dataset](https://archive.ics.uci.edu/dataset/2/adult). This is a classification task to predict if people made over $50k in the 1990's.\n",
    "\n",
    "**Model**\n",
    "We will train an XGBoost classifier model with default parameters for explanatory purposes. We are also performing a 50-50 train/test split for optimization purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data from shap library\n",
    "X,y = shap.datasets.adult()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below model may take a few minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jx1XzZUIJTDA"
   },
   "outputs": [],
   "source": [
    "# Train XGBoost model\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)\n",
    "model = xgboost.XGBClassifier()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b9WMrsxrSc4Z"
   },
   "source": [
    "## Partial Dependence Plots (PDP) 📈\n",
    "\n",
    "A Partial Dependence Plot (PDP or PD) shows the marginal effect one or two features have on the predicted outcome of a model [Paper, 2001](https://jerryfriedman.su.domains/ftp/trebst.pdf)\n",
    "\n",
    "**How it Works:**\n",
    "1. Select feature of interest\n",
    "2. For every instance in training dataset:\n",
    "* Keep all other features the same, create variants of the instance by replacing the feature’s value with values from a grid\n",
    "* Make predictions with the black box model for newly created instances\n",
    "* You now have a set of points for an instance with the feature value from the grid and the respective predictions\n",
    "3. Average across all instances and plot\n",
    "\n",
    "Here we will show both a built-in library implementation, [scikit learn's Partial Dependence Display](https://scikit-learn.org/stable/modules/generated/sklearn.inspection.PartialDependenceDisplay.html) and we will build our own implementation in numpy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ongpmbktTE6W"
   },
   "source": [
    "#### Using PartialDependenceDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 467
    },
    "id": "PzIBxd1eJs_S",
    "outputId": "78bf5e9b-7b74-4b13-aba3-ed74bfd4a5b1"
   },
   "outputs": [],
   "source": [
    "# Choose the feature of interest\n",
    "features = [\"Age\"]\n",
    "\n",
    "# Use PartialDependenceDisplay to plot PDP\n",
    "PartialDependenceDisplay.from_estimator(model, X_test, features, kind='average') #kind='both'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QgioG7rlTIUT"
   },
   "source": [
    "#### Build our own PDP with numpy\n",
    "\n",
    "When we build our own, you will notice we build our own grid of values and can change the size of our grid.\n",
    "\n",
    "This is why you may see slight variations between the PDP created with numpy versus the PartialDependenceDisplay from scikit learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "513bNGTyE8Ev",
    "outputId": "4647a4af-4521-4fdb-bac9-982d274c0ba8"
   },
   "outputs": [],
   "source": [
    "# Choose the feature for which you want to plot partial dependence\n",
    "feature_index = 0  # For example, the first feature\n",
    "\n",
    "# Create feature grid - here is where you can update the size of the grid by updating num\n",
    "feature_values = np.linspace(np.min(X.iloc[:, feature_index]), np.max(X.iloc[:, feature_index]), num=80)\n",
    "\n",
    "# Initialize array to store average predictions\n",
    "average_predictions = np.zeros_like(feature_values)\n",
    "\n",
    "# Duplicate the dataset to modify feature values\n",
    "X_modified = X.copy()\n",
    "\n",
    "# Loop over feature values\n",
    "for i, value in enumerate(feature_values):\n",
    "    # Set the chosen feature to the current value for all instances\n",
    "    X_modified.iloc[:, feature_index] = value\n",
    "\n",
    "    # Predict using the modified dataset\n",
    "    predictions = model.predict_proba(X_modified)[:, 1]\n",
    "\n",
    "    # Calculate average prediction for the current feature value\n",
    "    average_predictions[i] = np.mean(predictions)\n",
    "\n",
    "# Plot the partial dependence for the chosen feature\n",
    "plt.plot(feature_values, average_predictions)\n",
    "plt.xlabel(f'Feature {feature_index} values')\n",
    "plt.ylabel('Average predicted probability of class 1')\n",
    "plt.title(f'Partial Dependence for Feature {feature_index}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ba4nngrVTLBu"
   },
   "source": [
    "When we build our own PDP, we can run interesting experiments, like the one below, where we can see the impact of changing the size of our grid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SmxsJ2L0UITe"
   },
   "source": [
    "#### How to Interpret\n",
    "\n",
    "* Look at the shape of the curve on the plot. Is it linear, non-linear, or does it have any particular pattern? This gives you insights into how the feature affects the prediction.\n",
    "\n",
    "* Determine whether increasing or decreasing values of the feature variable lead to higher or lower predictions. Does the curve slope upwards, downwards, or remain relatively flat?\n",
    "\n",
    "* Note whether the curve reaches a plateau or has any upper or lower limits. This indicates whether there's a point beyond which changing the feature variable has little effect on the prediction.\n",
    "\n",
    "* Consider how the observed relationships align with your understanding of the problem domain. Are the results intuitive? Do they make sense based on what you know about the data?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "1wFguCx7PU8a",
    "outputId": "39f57dda-892e-4f2d-bc6a-39eecd4c0b07"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import to_rgba\n",
    "\n",
    "grid_length = np.linspace(20, 120, 40)\n",
    "fig, ax = plt.subplots(figsize=(8, 6))  # Create a figure and axis for plotting\n",
    "\n",
    "# Define a base color and alpha (transparency) values\n",
    "base_color = (0.2, 0.4, 0.6)  # Blue-ish color\n",
    "min_alpha = 0.2\n",
    "max_alpha = 1.0\n",
    "\n",
    "# Normalize grid_length values between 0 and 1\n",
    "normalized_g = (grid_length - grid_length.min()) / (grid_length.max() - grid_length.min())\n",
    "\n",
    "for i, g in enumerate(grid_length):\n",
    "    # Create feature grid\n",
    "    feature_values = np.linspace(np.min(X.iloc[:, feature_index]), np.max(X.iloc[:, feature_index]), num=int(g))\n",
    "    # Initialize array to store average predictions\n",
    "    average_predictions = np.zeros_like(feature_values)\n",
    "    # Duplicate the dataset to modify feature values\n",
    "    X_modified = X.copy()\n",
    "    # Loop over feature values\n",
    "    for j, value in enumerate(feature_values):\n",
    "        # Set the chosen feature to the current value for all instances\n",
    "        X_modified.iloc[:, feature_index] = value\n",
    "        # Predict using the modified dataset\n",
    "        predictions = model.predict_proba(X_modified)[:, 1]\n",
    "        # Calculate average prediction for the current feature value\n",
    "        average_predictions[j] = np.mean(predictions)\n",
    "    # Calculate color based on normalized g\n",
    "    alpha = min_alpha + (max_alpha - min_alpha) * normalized_g[i]\n",
    "    color = to_rgba(base_color, alpha)\n",
    "    # Plot the partial dependence for the chosen feature\n",
    "    ax.plot(feature_values, average_predictions, color=color)\n",
    "\n",
    "ax.set_xlabel(f'Feature {feature_index} values')\n",
    "ax.set_ylabel('Average predicted probability of class 1')\n",
    "ax.set_title(f'Partial Dependence for Feature {feature_index}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H4na5VBGOath"
   },
   "source": [
    "## ALE Plots 📉\n",
    "\n",
    "Accumulated Local Effects (ALE) Plots [Paper, 2020](https://www.scholars.northwestern.edu/en/publications/visualizing-the-effects-of-predictor-variables-in-black-box-super)\n",
    "\n",
    "**How to create an ALE plot:**\n",
    "1. Bin the Feature: Divide the feature of interest into several intervals (bins). These bins help in managing the data and computing local effects within smaller, more manageable segments.\n",
    "2. Compute Local Effects: For each bin, calculate the local effect of the feature on the prediction. This involves: Calculating the change in prediction when moving from the lower to the upper edge of the bin\n",
    "Averaging this change over all instances that fall into that bin\n",
    "3. Accumulate Effects: Starting from the first bin, accumulate the local effects across all bins. Sum up the average effects sequentially to show how the feature influences the prediction as its value changes\n",
    "4. Centering: To make the plot more interpretable, center the accumulated effects around zero. Subtract the mean of the accumulated effects, which forces the interpretation to focus on deviations from the average prediction\n",
    "\n",
    "There are a few python implementations of ALE plots, here we show the [ALEPython implementation](https://github.com/blent-ai/ALEPython).\n",
    "\n",
    "The implementation is more complex and less intuitive than PDPs, with many hyperparameters, including:\n",
    "* **bins** - This parameter defines the number of bins to divide the range of the specified feature into. A larger number of bins can provide more granularity in the ALE plot but might also increase computation time.\n",
    "* **monte_carlo** - This parameter is a boolean flag indicating whether to use Monte Carlo sampling to estimate the ALE. Monte Carlo sampling can be beneficial when the number of samples in the dataset is large, as it reduces computational burden.\n",
    "* **monte_carlo_rep** - This parameter specifies the number of Monte Carlo replicates to use for estimating the ALE. More replicates can lead to a more accurate estimation but may also increase computation time.\n",
    "* **monte_carlo_ratio** - This parameter determines the proportion of the dataset to use for Monte Carlo sampling. It's a value between 0 and 1, where 1 means using the entire dataset. Using a smaller ratio can speed up computation but may introduce some sampling error.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aAxyYF6VOLDr"
   },
   "source": [
    "This can take a couple of minutes to run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 530
    },
    "id": "qRnv6MJGNX3A",
    "outputId": "5171e044-be9c-4fd9-e1b2-aacfadb59528"
   },
   "outputs": [],
   "source": [
    "# Use default parameters for 1D Main Effect ALE Plot\n",
    "ale_plot(model, X_train, 'Age', monte_carlo=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 530
    },
    "id": "NhfPbardQq__",
    "outputId": "8babed21-0f08-40b0-e75a-e21f9a7eda20"
   },
   "outputs": [],
   "source": [
    "# Change hyperparameters for 1D Main Effect ALE Plot\n",
    "ale_plot(\n",
    "    model,\n",
    "    X_train,\n",
    "    \"Age\",\n",
    "    bins=5,\n",
    "    monte_carlo=True,\n",
    "    monte_carlo_rep=30,\n",
    "    monte_carlo_ratio=0.5,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PWVgjNuaVAQ3"
   },
   "source": [
    "#### How to Interpret 1D Main Effect ALE Plot\n",
    "\n",
    "\n",
    "\n",
    "* X-axis represents feature values\n",
    "* Y-axis shows average effect on predictions\n",
    "* Each curve represents a feature's ALE. - Flat curves imply little impact; steep curves, significant impact\n",
    "* Upward curves: increasing feature value increases predictions; downward, the opposite\n",
    "* Steeper curves signify larger effects\n",
    "\n",
    "We can compare ALE plots to gauge relative feature importance.\n",
    "Features with steeper curves have larger impacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 548
    },
    "id": "y3L7Abs6Q5z_",
    "outputId": "72ae9b9d-897b-4046-8b2e-b1f59c9c290e"
   },
   "outputs": [],
   "source": [
    "# 2D Second-Order ALE Plot\n",
    "ale_plot(model, X_train, X_train.columns[:2], bins=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iR-WV8-fVkDS"
   },
   "source": [
    "#### How to Interpret - 2D Second-Order ALE Plot\n",
    "\n",
    "* Both axes represent the values of the two features being analyzed.\n",
    "* Each axis corresponds to one of the features.\n",
    "* The plot displays a surface where the height represents the average effect on predictions. Higher points indicate regions where the model tends to make higher predictions, and vice versa.\n",
    "* Patterns in the surface reveal how the joint behavior of the two features affects the model's predictions. Peaks or valleys suggest regions where the joint effect is particularly strong.\n",
    "* The direction of the slope indicates whether increasing one feature while holding the other constant tends to increase or decrease predictions. Steeper slopes represent larger effects, while flatter regions indicate smaller effects.\n",
    "\n",
    "\n",
    "We can compare the 2D Second-Order ALE Plot with individual ALE plots for each feature to understand how joint effects differ from marginal effects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SLIGY1TLbSVm"
   },
   "source": [
    "## Permutation Feature Importance\n",
    "\n",
    "The importance of a feature can be measured by calculating how much model’s prediction error increases after permuting the feature.\n",
    "* If shuffling a feature’s values increases the model error, the feature is important\n",
    "* If the model error doesn’t change after shuffling a feature’s values, a feature is considered unimportant\n",
    "\n",
    "First introduced for random forests [Paper, 2001](https://link.springer.com/article/10.1023/A:1010933404324). Updated to be model agnostic - renamed “model reliance” [Paper, 2018](https://arxiv.org/abs/1801.01489).\n",
    "\n",
    "**Process for Model Agnostic measure:**\n",
    "1. Input: Trained model 𝑓^, feature matrix 𝑋, target vector 𝑦, error measure 𝐿(𝑦,𝑓^).\n",
    "2. Estimate the original model error 𝑒𝑜𝑟𝑖𝑔=𝐿(𝑦,𝑓^(𝑋)) (e.g. mean squared error)\n",
    "3. For each feature 𝑗∈{1,...,𝑝}:\n",
    "* Generate feature matrix 𝑋𝑝𝑒𝑟𝑚 by permuting feature j in the data X. This breaks the association between feature j and true outcome y\n",
    "* Estimate error 𝑒𝑝𝑒𝑟𝑚=𝐿(𝑌,𝑓^(𝑋𝑝𝑒𝑟𝑚)) based on the predictions of the permuted data.\n",
    "* Calculate permutation feature importance as quotient 𝐹𝐼𝑗=𝑒𝑝𝑒𝑟𝑚/𝑒𝑜𝑟𝑖𝑔 or difference 𝐹𝐼𝑗=𝑒𝑝𝑒𝑟𝑚 - 𝑒𝑜𝑟𝑖𝑔\n",
    "4. Sort features by descending FI\n",
    "\n",
    "**Implementation in Python**\n",
    "Here we will demonstrate the implementation in the scikit-learn library.[[Documentation Here](https://scikit-learn.org/stable/modules/permutation_importance.html)]\n",
    "\n",
    "The scikit-learn implementation of the permutation_importance function calculates the feature importance of estimators for a given dataset. The n_repeats parameter sets the number of times a feature is randomly shuffled and returns a sample of feature importances.\n",
    "\n",
    "* n_repeats - number of times to permute a feature\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dw_Tv4oVcSro"
   },
   "outputs": [],
   "source": [
    "# Compute permutation importances\n",
    "perm_imp = permutation_importance(model, X_test, y_test,\n",
    "                           n_repeats=30,\n",
    "                           random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dptM10KNcxgT",
    "outputId": "4aac5df5-2236-4a1c-b1d4-33310820de9f"
   },
   "outputs": [],
   "source": [
    "# Print the mean and standard deviation of permutation importances for each feature\n",
    "\n",
    "for i in perm_imp.importances_mean.argsort()[::-1]:\n",
    "    if perm_imp.importances_mean[i] - 2 * perm_imp.importances_std[i] > 0:\n",
    "        print(f\"{X.columns[i]:<8}\"\n",
    "              f\"{perm_imp.importances_mean[i]:.3f}\"\n",
    "              f\" +/- {perm_imp.importances_std[i]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "CKXg4PsZeZgQ",
    "outputId": "5842d947-32b8-48df-c9b9-a79d54f2cf8a"
   },
   "outputs": [],
   "source": [
    "# Plot Permutation Feature Importances as a bar chart\n",
    "\n",
    "sorted_idx = perm_imp.importances_mean.argsort()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.barh(X_test.columns[sorted_idx], perm_imp.importances[sorted_idx].mean(axis=1).T)\n",
    "ax.set_title(\"Permutation Importances\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "l16dtncffAaZ",
    "outputId": "12788629-d477-490c-b6d7-7a70531b1979"
   },
   "outputs": [],
   "source": [
    "# Plot Permutation Feature Importances as a box plot\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.boxplot(perm_imp.importances[sorted_idx].T,\n",
    "           vert=False, labels=X_test.columns[sorted_idx])\n",
    "ax.set_title(\"Permutation Importances\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G8kN4O0lcITw"
   },
   "source": [
    "⚠️ Caution when using Permutation Feature Importance ⚠️\n",
    "\n",
    "Features that are deemed of low importance for a bad model (low cross-validation score) could be very important for a good model. Therefore it is always important to evaluate the predictive power of a model using a held-out set (or better with cross-validation) prior to computing importances. Permutation importance does not reflect to the intrinsic predictive value of a feature by itself but how important this feature is for a particular model. [Source](https://scikit-learn.org/stable/modules/permutation_importance.html#id2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fn1TE2O-aByw"
   },
   "source": [
    "## Feature Interaction\n",
    "\n",
    "Friedman's H-statistic is still being implemented in scikit-learn ([see PR here](https://github.com/scikit-learn/scikit-learn/pull/28375)). It is currently in progress ([code here](https://github.com/mayer79/scikit-learn/blob/friedmans-h/sklearn/inspection/_h_statistic.py))\n",
    "\n",
    "When this is added to scikit-learn, I will update this tutorial.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
