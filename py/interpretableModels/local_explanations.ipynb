{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yz_omiksko2M"
   },
   "source": [
    "# XAI CODE DEMO\n",
    "\n",
    "## Explainable AI Specialization on Coursera\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ASXFH-S88xMp"
   },
   "source": [
    "# Local Explanations üèòÔ∏è\n",
    "\n",
    "If high latency occurs when running this notebook in Coursera, feel free to Open in Google Colab:\n",
    "\n",
    "[![Open In Collab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/explainable-machine-learning/explainable-ml/blob/main/local_explanations.ipynb)\n",
    "\n",
    "\n",
    "Table of Contents\n",
    "* [LIME üçã](https://colab.research.google.com/drive/1aBZSreILCbF3x7f5PIIlA3eNTmxoyzL7#scrollTo=nTO5RNakdVPW&line=19&uniqifier=1)\n",
    "* [Anchors ‚öìÔ∏è](https://colab.research.google.com/drive/1aBZSreILCbF3x7f5PIIlA3eNTmxoyzL7#scrollTo=CzDNNmMrdizl&line=17&uniqifier=1)\n",
    "* [SHAP üé≤](https://colab.research.google.com/drive/1aBZSreILCbF3x7f5PIIlA3eNTmxoyzL7#scrollTo=kRWoDl1SdQbK&line=22&uniqifier=1)\n",
    "* [ICE plots üßä](https://colab.research.google.com/drive/1aBZSreILCbF3x7f5PIIlA3eNTmxoyzL7#scrollTo=8WVOuglNdgEJ&line=9&uniqifier=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YcSkSBv8arP2"
   },
   "source": [
    "First, we will install the necessary libraries. In addition to numpy and pandas, we will use the following libraries:\n",
    "\n",
    "\n",
    "*   scikit-learn (Model, ICE plots)\n",
    "*   shap (SHAP)\n",
    "*   lime (LIME)\n",
    "*   anchor-exp (Anchors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WvkxtQjkPE7r",
    "outputId": "1c3d06c7-cf6c-4dc4-a3d6-fa8470c7e963"
   },
   "outputs": [],
   "source": [
    "!pip install numpy==1.25.2 pandas==2.0.3 matplotlib==3.7.1 scikit-learn==1.2.2 lime==0.2.0.1 anchor-exp==0.0.2.0 shap==0.45.1 xgboost==1.7.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s37tgMnycl_c"
   },
   "outputs": [],
   "source": [
    "# Data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Models\n",
    "import xgboost\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# XAI\n",
    "import shap\n",
    "import lime\n",
    "from anchor import anchor_tabular\n",
    "from sklearn.inspection import PartialDependenceDisplay\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZvKGriyCbq6K"
   },
   "source": [
    "**Dataset**\n",
    "We will use the classic [UCI adult income dataset](https://archive.ics.uci.edu/dataset/2/adult). This is a classification task to predict if people made over $50k in the 1990's.\n",
    "\n",
    "**Model**\n",
    "We will train an XGBoost classifier model with default parameters for explanatory purposes. We are also performing a 50-50 train/test split for optimization purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J8U-jA1wbg2L"
   },
   "outputs": [],
   "source": [
    "# Load Data from shap library\n",
    "X,y = shap.datasets.adult()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model being trained below may take a few minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)\n",
    "\n",
    "# Train XGBoost model\n",
    "model = xgboost.XGBClassifier()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jxRukAoOiKiT"
   },
   "source": [
    "For LIME and Anchors, we need to define some characteristics of our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-MQdWFbziJ0H"
   },
   "outputs": [],
   "source": [
    "feature_names = [\"Age\", \"Workclass\",\n",
    "                 \"Education-Num\", \"Marital Status\", \"Occupation\",\n",
    "                 \"Relationship\", \"Race\", \"Sex\", \"Capital Gain\",\n",
    "                 \"Capital Loss\", \"Hours per week\", \"Country\"]\n",
    "\n",
    "categorical_features = [\"Workclass\", \"Marital Status\", \"Occupation\", \"Relationship\", \"Race\", \"Sex\", \"Country\"]\n",
    "\n",
    "class_names = ['<=50K', '>50K']\n",
    "\n",
    "categorical_names = {\n",
    "    1: ['Private', 'Self-emp-not-inc', 'Self-emp-inc', 'Federal-gov', 'Local-gov', 'State-gov', 'Without-pay', 'Never-worked'],  # Workclass\n",
    "    3: ['Married-civ-spouse', 'Divorced', 'Never-married', 'Separated', 'Widowed', 'Married-spouse-absent', 'Married-AF-spouse'],  # Marital Status\n",
    "    4: ['Tech-support', 'Craft-repair', 'Other-service', 'Sales', 'Exec-managerial', 'Prof-specialty', 'Handlers-cleaners', 'Machine-op-inspct', 'Adm-clerical', 'Farming-fishing', 'Transport-moving', 'Priv-house-serv', 'Protective-serv', 'Armed-Forces'],  # Occupation\n",
    "    5: ['Wife', 'Own-child', 'Husband', 'Not-in-family', 'Other-relative', 'Unmarried'],  # Relationship\n",
    "    6: ['White', 'Asian-Pac-Islander', 'Amer-Indian-Eskimo', 'Other', 'Black'],  # Race\n",
    "    7: ['Female', 'Male'],  # Sex\n",
    "    11: ['United-States', 'Cambodia', 'England', 'Puerto-Rico', 'Canada', 'Germany', 'Outlying-US(Guam-USVI-etc)', 'India', 'Japan', 'Greece', 'South', 'China', 'Cuba', 'Iran', 'Honduras', 'Philippines', 'Italy', 'Poland', 'Jamaica', 'Vietnam', 'Mexico', 'Portugal', 'Ireland', 'France', 'Dominican-Republic', 'Laos', 'Ecuador', 'Taiwan', 'Haiti', 'Columbia', 'Hungary', 'Guatemala', 'Nicaragua', 'Scotland', 'Thailand', 'Yugoslavia', 'El-Salvador', 'Trinadad&Tobago', 'Peru', 'Hong', 'Holand-Netherlands']  # Country\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nTO5RNakdVPW"
   },
   "source": [
    "## LIME üçã\n",
    "\n",
    "Local interpretable model-agnostic explanations (LIME) [[Paper, 2016]](https://arxiv.org/abs/1602.04938)\n",
    "\n",
    "Interpretable models that are used to explain individual predictions of black box machine learning model\n",
    "\n",
    "**LIME Process:**\n",
    "1.   Select instance of interest\n",
    "2.   Perturb your dataset and get black box predictions for perturbed samples\n",
    "3.   Generate a new dataset consisting of perturbed samples (variations of your data) and the corresponding predictions\n",
    "4.   Train an interpretable model, weighted by the proximity of sampled instances to the instance of interest\n",
    "5.   Interpret the local model to explain prediction\n",
    "\n",
    "As opposed to LIME for text, tabular explainers need a training set. This is because statistics are computed on each feature. If the feature is numerical, we compute the mean and std, and discretize it into quartiles. If the feature is categorical, we compute the frequency of each value. This is used to scale the data, so that we can meaningfully compute distances when the attributes are not on the same scale and to sample perturbed instances - which we do by sampling from a Normal(0,1), multiplying by the std and adding back the mean.\n",
    "\n",
    "\n",
    "LIME uses an exponential smoothing kernel to define the neighborhood.\n",
    "The kernel width determines how large the neighborhood is:\n",
    "* Small kernel width = an instance must be very close to influence the local model\n",
    "* Larger kernel width = instances that are farther away also influence the model\n",
    "\n",
    "[LIME Code Tutorial from original paper authors](https://marcotcr.github.io/lime/tutorials/Tutorial%20-%20continuous%20and%20categorical%20features.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 389
    },
    "id": "TBoIW4G-gpC9",
    "outputId": "23cf2bad-6b94-4f21-f801-4340cdf25d0b"
   },
   "outputs": [],
   "source": [
    "# Define kernel_width\n",
    "kernel_width = 3\n",
    "\n",
    "# Initialize LIME explainer\n",
    "explainer = lime.lime_tabular.LimeTabularExplainer(X_train.values ,class_names=class_names, feature_names=feature_names,\n",
    "                                                   categorical_features=categorical_features,\n",
    "                                                   categorical_names=categorical_names, kernel_width=kernel_width)\n",
    "# Choose a sample for explanation\n",
    "idx = 0\n",
    "\n",
    "# Explain the prediction using LIME\n",
    "exp = explainer.explain_instance(X_test.values[idx], model.predict_proba, num_features=12)\n",
    "\n",
    "# Show the explanation\n",
    "exp.show_in_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2t76RYvC94fT"
   },
   "source": [
    "### How to Interpret\n",
    "\n",
    "On the left, the prediction probability is shown. This sample was predicted to be <=50K (this individual in the dataset was predicted to make less than $50k)\n",
    "\n",
    "For this sample, features that contributed to the prediction of <=50k are shown in blue (left) and features that contributed to a prediction of >50k are shown in orange (right).\n",
    "\n",
    "The table on the right shows the actual value for each feature for this particular instance and the feature is highlighted with its contribution to each of the binary outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 389
    },
    "id": "fp0HgBmPjTvC",
    "outputId": "53b70899-7023-4cb3-e75e-4654311e7d84"
   },
   "outputs": [],
   "source": [
    "# Choose a sample for explanation\n",
    "idx = 100\n",
    "\n",
    "# Explain the prediction using LIME\n",
    "exp = explainer.explain_instance(X_test.values[idx], model.predict_proba, num_features=12)\n",
    "\n",
    "# Show the explanation\n",
    "exp.show_in_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8iMrmlCSfRpI"
   },
   "source": [
    "‚ö†Ô∏è Warning about using LIME for tabular data ‚ö†Ô∏è\n",
    "\n",
    "* In tabular data, LIME samples are not taken around the instance of interest, but from the training data‚Äôs mass center\n",
    "\n",
    "* Defining the neighborhood around a point is difficult and can change the results of your local model\n",
    "\n",
    "* There is not yet a robust way to find the best kernel or width\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CzDNNmMrdizl"
   },
   "source": [
    "## Anchors ‚öìÔ∏è\n",
    "\n",
    "High-Precision Model-Agnostic Explanations [Paper, 2018](https://ojs.aaai.org/index.php/AAAI/article/view/11491)\n",
    "\n",
    "* Explains individual predictions by finding a decision rule that sufficiently ‚Äúanchors‚Äù the prediction\n",
    "* Anchors are easy to understand IF-THEN rules (rather than interpretable models, like in LIME)\n",
    "\n",
    "Components to Finding Anchors:\n",
    "Candidate Generation - generates new explanation candidates\n",
    "\n",
    "1.   Identify the Best Candidate - Which rule explains x best?\n",
    "2.   Multi-Armed-Bandit - sequential selection method\n",
    "3.   Candidate Precision Validation - Takes more samples when there is no statistical confidence that the candidate exceeds the ùúè threshold\n",
    "4.   Modified Beam Search - graph search algorithm\n",
    "Carries the B best candidates of each round over to the n\n",
    "\n",
    "[Anchors Code Tutorial from original paper authors](https://github.com/marcotcr/anchor/blob/master/notebooks/Anchor%20on%20tabular%20data.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MVth7yziQIsX"
   },
   "outputs": [],
   "source": [
    "# Initialize Anchors explainer\n",
    "explainer = anchor_tabular.AnchorTabularExplainer(\n",
    "    class_names,\n",
    "    feature_names,\n",
    "    X_train.values,\n",
    "    categorical_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Id4u2unvTzE-",
    "outputId": "a034e6cd-4aa4-494a-ad51-930b9ea58208"
   },
   "outputs": [],
   "source": [
    "# Choose a sample for explanation\n",
    "idx = 100\n",
    "\n",
    "# Print Prediction\n",
    "print('Prediction: ', explainer.class_names[model.predict(X_test.values[idx].reshape(1, -1))[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9Kmz8waDW61-"
   },
   "outputs": [],
   "source": [
    "# Explain the prediction using Anchors\n",
    "exp = explainer.explain_instance(X_test.values[idx], model.predict, threshold=0.80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_jy8_1pWj_Gk"
   },
   "source": [
    "**Coverage** = an anchor‚Äôs probability of applying to its neighbors\n",
    "* There is a trade-off between coverage and precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NSdO1Af9XAxl",
    "outputId": "490c779c-893b-42cc-9db1-93a991fb22ff"
   },
   "outputs": [],
   "source": [
    "# Print the prediction, precision, and coverage\n",
    "print('Anchor: %s' % (' AND '.join(exp.names())))\n",
    "print('Precision: %.2f' % exp.precision())\n",
    "print('Coverage: %.2f' % exp.coverage())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "55Zu2T6I-age"
   },
   "source": [
    "### How to Interpret\n",
    "\n",
    "For this particular example, the Anchors rule created for this particular instance:\n",
    "\n",
    "Capital Gain > 0 AND Relationship = Other-relative AND Age > 28 AND Marital Status = Never-married AND Education-Num>10 AND Sex=Male\n",
    "\n",
    "We print the precision and coverage, which are 0.73 and 0.02, respectively. Keep in mind the trade-off between coverage and precision.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kRWoDl1SdQbK"
   },
   "source": [
    "## SHAP üé≤\n",
    "\n",
    "SHapley Additive exPlanations [Paper, 2017](https://papers.nips.cc/paper_files/paper/2017/file/8a20a8621978632d76c43dfd28b67767-Paper.pdf)\n",
    "\n",
    "\n",
    "Builds off of Shapley Values by approximating them.\n",
    "\n",
    "* Shapley Values is a method from coalitional game theory (where game players cooperate in a coalition) that tells us how to fairly distribute ‚Äúpayout‚Äù to players depending on their contribution to the total payout [Paper, 1952](https://www.rand.org/content/dam/rand/pubs/papers/2021/P295.pdf)\n",
    "* For our purposes, we will assume the prediction task is the ‚Äúgame‚Äù, each feature value of the instance is a ‚Äúplayer‚Äù, and the prediction is the ‚Äúpayout‚Äù\n",
    "\n",
    "The `shap` library has great [documentation](https://shap.readthedocs.io/en/latest/) with excellent tutorials.\n",
    "\n",
    "**The role of the background distribution:**\n",
    "\n",
    "* *Baseline Value Calculation*: SHAP values explain a prediction by comparing it to a baseline value, which is typically the average prediction over the background dataset. This baseline serves as a reference point to understand how much each feature contributes to the difference between the actual prediction and this average prediction. Below, you will see this variable as `X100`.\n",
    "\n",
    "* *Expectation of Model Output*: When calculating SHAP values, the background dataset is used to estimate the expected value of the model's output. This expected value is the average prediction across the background dataset and represents what the model would predict in the absence of specific information about a given instance.\n",
    "\n",
    "* *Marginal Contribution of Features*: SHAP values measure the marginal contribution of each feature to the prediction. This involves considering what the model would predict if each feature were missing or replaced by values from the background distribution. Essentially, SHAP values quantify how much each feature contributes to the difference between the actual prediction and the baseline value derived from the background dataset.\n",
    "\n",
    "* *Sampling and Computational Efficiency*: Using a background dataset allows SHAP to approximate the impact of removing a feature by averaging over many possible values that the feature could take. This is more computationally efficient than considering all possible combinations of feature values.\n",
    "\n",
    "...\n",
    "\n",
    "Below, we implement the **TreeSHAP algorithm**.\n",
    "You can read more about the TreeSHAP algorithm in the [Interpretable ML Book](https://christophm.github.io/interpretable-ml-book/shap.html#treeshap)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ESZ_YGCtVaib",
    "outputId": "64a46b6c-92bc-46f2-d194-283dbb9600b2"
   },
   "outputs": [],
   "source": [
    "# Compute SHAP values\n",
    "\n",
    "X100 = shap.utils.sample(X, 100) # 100 instances for use as the background distribution\n",
    "\n",
    "explainer = shap.TreeExplainer(model, X100) # Use the TreeExplainer algorithm with background distribution\n",
    "shap_values = explainer.shap_values(X_test) # Get shap values\n",
    "shap_values_exp = explainer(X_test) # Get explainer for X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 637
    },
    "id": "wXJSslzzVtvx",
    "outputId": "6f91f291-a81b-4e4d-8d60-4ef3c357a2e8"
   },
   "outputs": [],
   "source": [
    "# Visualize SHAP with Summary Plot\n",
    "shap.summary_plot(shap_values, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ivd7oOy2_EFT"
   },
   "source": [
    "### How to Interpret - SHAP Summary Plot\n",
    "\n",
    "The Summary Plot is a beeswarm plot. We can see the features listed on the left side of the plot. The bottom shows the SHAP value, which is the impact on the model output.\n",
    "\n",
    "* For classification models, especially binary classifiers, a negative SHAP value indicates that the feature is contributing to lowering the probability of the positive class, or increasing the probability of the negative class, (<=50k).\n",
    "\n",
    "* Postive SHAP values indicate that the feature is contributing to increasing the probability of the positive class (>50k)\n",
    "\n",
    "* Whether the value of the feature is low or high is shown by the color, with low feature values in blue and high feature values shown in red. This is normalized by feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 637
    },
    "id": "evUu3vK-AHbD",
    "outputId": "d17844dd-11d4-4ef0-acb9-2df94fff2237"
   },
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values, X_test, plot_type=\"bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_47diKj3AO2k"
   },
   "source": [
    "### How to Interpret - SHAP Bar chart\n",
    "\n",
    "This plot takes the average of the SHAP value magnitudes across the dataset and plots it as a simple bar chart.\n",
    "\n",
    "We can visualize the largest contributors more globally with this plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 476
    },
    "id": "RANshIoyVe_5",
    "outputId": "f5f32a00-fdec-4fcc-e5ea-3a44d7704f20"
   },
   "outputs": [],
   "source": [
    "# Visualize SHAP with Dependence Plot\n",
    "shap.dependence_plot('Age', shap_values, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sJXcSoLy_0E4"
   },
   "source": [
    "### How to Interpret - SHAP Dependence Plot\n",
    "\n",
    "SHAP dependence plots show the effect of a single feature across the whole dataset. They plot a feature‚Äôs value vs. the SHAP value of that feature across many samples. SHAP dependence plots may look similar to partial dependence plots, but account for the interaction effects present in the features, and are only defined in regions of the input space supported by data. The vertical dispersion of SHAP values at a single feature value is driven by interaction effects, and another feature is chosen for coloring to highlight possible interactions.\n",
    "[Source](https://shap-lrjball.readthedocs.io/en/latest/example_notebooks/tree_explainer/Census%20income%20classification%20with%20XGBoost.html#SHAP-Dependence-Plots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 617
    },
    "id": "nnBM2h7utmBo",
    "outputId": "01ffbe54-a7a1-4504-b75e-3b56ecec1a56"
   },
   "outputs": [],
   "source": [
    "# Visualize an instance with a waterfall plot of the SHAP values\n",
    "\n",
    "# Choose a sample for explanation\n",
    "idx = 100\n",
    "\n",
    "# Plot waterfall plot\n",
    "shap.plots.waterfall(shap_values_exp[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NWOic04hBAPk"
   },
   "source": [
    "#### How to Interpret - SHAP Waterfall Plot\n",
    "\n",
    "You can visualize the SHAP values for an instance of interest. For an instance of interest, you can see the SHAP values and their contribution to the prediction for that instance.\n",
    "\n",
    "On the left side of the feature names are the specific feature values for the instance of interest\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8WVOuglNdgEJ"
   },
   "source": [
    "## ICE Plots üßä\n",
    "\n",
    "Individual Conditional Expectation (ICE) plots one line per instance that displays how the instance‚Äôs prediction changes when a feature changes [Paper, 2014](https://arxiv.org/pdf/1309.6392)\n",
    "\n",
    "**How it Works**\n",
    "1. Select instance and feature of interest\n",
    "2. Keep all other features the same, create variants of the instance by replacing the feature‚Äôs value with values from a grid\n",
    "3. Make predictions with the black box model for newly created instances\n",
    "4. You now have a set of points for an instance with the feature value from the grid and the respective predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 467
    },
    "id": "XgMwfy7mKZ9w",
    "outputId": "ebd19332-f792-41f3-e7c8-cc0acaecd4ab"
   },
   "outputs": [],
   "source": [
    "# Choose the feature of interest\n",
    "features = [\"Age\"]\n",
    "\n",
    "# Use PartialDependenceDisplay to display the ICE plot\n",
    "PartialDependenceDisplay.from_estimator(model, X_test, features, kind='individual') #kind='individual'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zS19oKUE780n"
   },
   "source": [
    "#### Partial Dependence Plot (Global Explanation üåé)\n",
    "\n",
    "A Partial Dependence Plot (PDP or PD) shows the marginal effect one or two features have on the predicted outcome of a model [Paper, 2001](https://jerryfriedman.su.domains/ftp/trebst.pdf)\n",
    "\n",
    "* It is the average of the lines of an ICE plot!\n",
    "* Can show the relationship between a feature and the target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 467
    },
    "id": "bgTLZwr3NUhm",
    "outputId": "03713bde-f411-4915-95cf-770e1e576e31"
   },
   "outputs": [],
   "source": [
    "# Use PartialDependenceDisplay to display the ICE plot and the PDP overlayed on top\n",
    "\n",
    "PartialDependenceDisplay.from_estimator(model, X_test, features, kind='both') #kind='both'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pvJUbZGJ7pwj"
   },
   "source": [
    "#### Centered ICE plots (c-ICE) ‚öñÔ∏è\n",
    "We can center the curves at a certain point in the feature and display only the difference in the prediction to this point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 467
    },
    "id": "XzZDzjTXKuj0",
    "outputId": "275f1c40-794a-4d96-ddc2-83c5ebb774a7"
   },
   "outputs": [],
   "source": [
    "PartialDependenceDisplay.from_estimator(model, X_test, features, kind='both', centered=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C3Q3cpWbFd91"
   },
   "source": [
    "#### ICE Plot using numpy\n",
    "\n",
    "We can also use numpy to compute ICE plots. Scikit-learn does not support looking at individual instances in their PartialDependenceDisplay, so if we want to look at individual instances rather than all of the instances, we will need to code the plots ourselves using numpy and matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "gvzbFGQNEwmR",
    "outputId": "faa86a6f-cc33-4c17-dc2c-e360665c71f2"
   },
   "outputs": [],
   "source": [
    "# Choose the instance and feature for which you want to plot the ICE plot\n",
    "instance_index = 0  # Choose the index of the instance you want to visualize\n",
    "feature_index = 0  # Let's look at \"Age\"\n",
    "\n",
    "# Create feature grid\n",
    "feature_values = np.linspace(np.min(X.iloc[:, feature_index]), np.max(X.iloc[:, feature_index]), num=50)\n",
    "\n",
    "# Initialize array to store average predictions\n",
    "average_predictions = np.zeros_like(feature_values)\n",
    "\n",
    "# Extract the instance of interest\n",
    "instance = X.iloc[[instance_index]]\n",
    "\n",
    "# Duplicate the instance to modify feature values\n",
    "instance_modified = instance.copy()\n",
    "\n",
    "# Loop over feature values\n",
    "for i, value in enumerate(feature_values):\n",
    "    # Set the chosen feature to the current value for the instance:\n",
    "    instance_modified.iloc[:, feature_index] = value\n",
    "\n",
    "    # Predict using the modified instance:\n",
    "    prediction = model.predict_proba(instance_modified)[:, 1]\n",
    "\n",
    "    # Store the prediction for the current feature value:\n",
    "    average_predictions[i] = prediction.item()\n",
    "\n",
    "# Plot the ICE plot\n",
    "plt.plot(feature_values, average_predictions)\n",
    "plt.xlabel(f'Feature values')\n",
    "plt.ylabel('Predicted probability of class 1')\n",
    "plt.title(f'ICE Plot - Instance Index: {instance_index}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VanLpmHWGOKV"
   },
   "source": [
    "#### How to Interpret - ICE Plot\n",
    "\n",
    "For the instance demonstrated above, as age increases, the predicted probability of >=50k increases. There is interesting behavior around 80 years old for this instance.\n",
    "\n",
    "Try another instance yourself to see how the ICE plot and interpretation changes across instances!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
