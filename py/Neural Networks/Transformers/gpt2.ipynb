{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T12:37:36.216319Z",
     "start_time": "2024-06-20T12:37:36.202808Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mImportError: /home/haroon/.conda/envs/ml/lib/python3.12/lib-dynload/_sqlite3.cpython-312-x86_64-linux-gnu.so: undefined symbol: sqlite3_deserialize. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# import tinycss2.tokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dataclasses import dataclass\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9342887e02f8e2ad",
   "metadata": {},
   "source": [
    "In GPT positional embedding were fixed to sin/cos of varying freq. but for GPT2 they are params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f0718b4dc069b11",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T12:37:38.627284Z",
     "start_time": "2024-06-20T12:37:36.242900Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"Hello World:\\n\\nThis is an awesome story written by one of our favorite anime-loving fan, and I would love to see it translated into English! Also, thanks for stopping by! I'm also a fan of the show so I can\"}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline,set_seed\n",
    "\n",
    "generator = pipeline('text-generation', model='gpt2')\n",
    "set_seed(42)\n",
    "generator(\"Hello World\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd41cc223bc10f60",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T12:37:39.949797Z",
     "start_time": "2024-06-20T12:37:38.627284Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformer.wte.weight torch.Size([50257, 768])\n",
      "transformer.wpe.weight torch.Size([1024, 768])\n",
      "transformer.h.0.ln_1.weight torch.Size([768])\n",
      "transformer.h.0.ln_1.bias torch.Size([768])\n",
      "transformer.h.0.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.0.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.0.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.0.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.0.ln_2.weight torch.Size([768])\n",
      "transformer.h.0.ln_2.bias torch.Size([768])\n",
      "transformer.h.0.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.0.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.0.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.0.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.1.ln_1.weight torch.Size([768])\n",
      "transformer.h.1.ln_1.bias torch.Size([768])\n",
      "transformer.h.1.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.1.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.1.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.1.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.1.ln_2.weight torch.Size([768])\n",
      "transformer.h.1.ln_2.bias torch.Size([768])\n",
      "transformer.h.1.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.1.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.1.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.1.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.2.ln_1.weight torch.Size([768])\n",
      "transformer.h.2.ln_1.bias torch.Size([768])\n",
      "transformer.h.2.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.2.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.2.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.2.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.2.ln_2.weight torch.Size([768])\n",
      "transformer.h.2.ln_2.bias torch.Size([768])\n",
      "transformer.h.2.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.2.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.2.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.2.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.3.ln_1.weight torch.Size([768])\n",
      "transformer.h.3.ln_1.bias torch.Size([768])\n",
      "transformer.h.3.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.3.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.3.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.3.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.3.ln_2.weight torch.Size([768])\n",
      "transformer.h.3.ln_2.bias torch.Size([768])\n",
      "transformer.h.3.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.3.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.3.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.3.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.4.ln_1.weight torch.Size([768])\n",
      "transformer.h.4.ln_1.bias torch.Size([768])\n",
      "transformer.h.4.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.4.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.4.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.4.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.4.ln_2.weight torch.Size([768])\n",
      "transformer.h.4.ln_2.bias torch.Size([768])\n",
      "transformer.h.4.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.4.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.4.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.4.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.5.ln_1.weight torch.Size([768])\n",
      "transformer.h.5.ln_1.bias torch.Size([768])\n",
      "transformer.h.5.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.5.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.5.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.5.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.5.ln_2.weight torch.Size([768])\n",
      "transformer.h.5.ln_2.bias torch.Size([768])\n",
      "transformer.h.5.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.5.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.5.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.5.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.6.ln_1.weight torch.Size([768])\n",
      "transformer.h.6.ln_1.bias torch.Size([768])\n",
      "transformer.h.6.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.6.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.6.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.6.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.6.ln_2.weight torch.Size([768])\n",
      "transformer.h.6.ln_2.bias torch.Size([768])\n",
      "transformer.h.6.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.6.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.6.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.6.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.7.ln_1.weight torch.Size([768])\n",
      "transformer.h.7.ln_1.bias torch.Size([768])\n",
      "transformer.h.7.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.7.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.7.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.7.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.7.ln_2.weight torch.Size([768])\n",
      "transformer.h.7.ln_2.bias torch.Size([768])\n",
      "transformer.h.7.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.7.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.7.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.7.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.8.ln_1.weight torch.Size([768])\n",
      "transformer.h.8.ln_1.bias torch.Size([768])\n",
      "transformer.h.8.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.8.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.8.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.8.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.8.ln_2.weight torch.Size([768])\n",
      "transformer.h.8.ln_2.bias torch.Size([768])\n",
      "transformer.h.8.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.8.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.8.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.8.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.9.ln_1.weight torch.Size([768])\n",
      "transformer.h.9.ln_1.bias torch.Size([768])\n",
      "transformer.h.9.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.9.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.9.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.9.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.9.ln_2.weight torch.Size([768])\n",
      "transformer.h.9.ln_2.bias torch.Size([768])\n",
      "transformer.h.9.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.9.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.9.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.9.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.10.ln_1.weight torch.Size([768])\n",
      "transformer.h.10.ln_1.bias torch.Size([768])\n",
      "transformer.h.10.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.10.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.10.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.10.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.10.ln_2.weight torch.Size([768])\n",
      "transformer.h.10.ln_2.bias torch.Size([768])\n",
      "transformer.h.10.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.10.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.10.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.10.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.11.ln_1.weight torch.Size([768])\n",
      "transformer.h.11.ln_1.bias torch.Size([768])\n",
      "transformer.h.11.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.11.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.11.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.11.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.11.ln_2.weight torch.Size([768])\n",
      "transformer.h.11.ln_2.bias torch.Size([768])\n",
      "transformer.h.11.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.11.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.11.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.11.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.ln_f.weight torch.Size([768])\n",
      "transformer.ln_f.bias torch.Size([768])\n",
      "lm_head.weight torch.Size([50257, 768])\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "for k,v in model.state_dict().items():\n",
    "  print(k,v.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2bb54da450e1b94",
   "metadata": {},
   "source": [
    "From scratch\n",
    "\n",
    "<img src=\"model.png\" alt=model>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "966edae7581db048",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T13:05:03.065133Z",
     "start_time": "2024-06-20T13:05:03.038136Z"
    }
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "  def __init__(self, config):\n",
    "    super().__init__()\n",
    "    self.c_fc = nn.Linear(config.n_embd, 4*config.n_embd)\n",
    "    self.gelu = nn.GELU()\n",
    "      #GELU(approximate='tanh')  # gaussian Rectifier approx by tanh (original was slow in tf)\n",
    "    self.c_proj = nn.Linear(4*config.n_embd, config.n_embd)\n",
    "      \n",
    "  def forward(self, x):\n",
    "    x = self.c_fc(x)\n",
    "    x = self.gelu(x)\n",
    "    x = self.c_proj(x)\n",
    "    return x\n",
    "  \n",
    "class CausalSelfAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        # key, query, value projections for all heads, but in a batch\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
    "        # regularization\n",
    "        self.attn_dropout = nn.Dropout(config.dropout)\n",
    "        self.resid_dropout = nn.Dropout(config.dropout)\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.dropout = config.dropout\n",
    "        # flash attention make GPU go brrrrr but support is only in PyTorch >= 2.0\n",
    "        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')\n",
    "        if not self.flash:\n",
    "            print(\"WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\")\n",
    "            # causal mask to ensure that attention is only applied to the left in the input sequence\n",
    "            self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size).view(1, 1, config.block_size, config.block_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "\n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        q, k, v  = self.c_attn(x).split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "\n",
    "        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
    "        if self.flash:\n",
    "            # efficient attention using Flash Attention CUDA kernels\n",
    "            y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True)\n",
    "        else:\n",
    "            # manual implementation of attention\n",
    "            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "            att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
    "            att = F.softmax(att, dim=-1)\n",
    "            att = self.attn_dropout(att)\n",
    "            y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "\n",
    "        # output projection\n",
    "        y = self.resid_dropout(self.c_proj(y))\n",
    "        return y\n",
    "\n",
    "class Block(nn.Module): # transformer block\n",
    "  def __init__(self, config):\n",
    "    super().__init__()\n",
    "    self.ln_1 = nn.LayerNorm(config.n_embd)  \n",
    "    self.attn = CausalSelfAttention(config)\n",
    "    self.ln_2 = nn.LayerNorm(config.n_embd)  \n",
    "    self.mlp = MLP(config)\n",
    "  \n",
    "  def forward(self, x):\n",
    "    x = x + self.attn(self.ln_1(x))  # residual connections according to diagram plus ln after attn i.e. ln in residuals\n",
    "    x = x + self.mlp(self.ln_2(x))\n",
    "    return x\n",
    "      \n",
    "@dataclass\n",
    "class GPTConfig:   # gpt 124M\n",
    "  block_size = 1024   # max sequence length\n",
    "  vocab_size = 50257  # 50k BPE + 256 utf-8 + 1 <|endoftext|>\n",
    "  n_layer = 12        \n",
    "  n_head = 12\n",
    "  n_embd = 768\n",
    "  dropout = 0.2\n",
    "  bias = True\n",
    "\n",
    "class GPT(nn.Module):\n",
    "  def __init__(self, config):\n",
    "    super().__init__()\n",
    "    self.config = config\n",
    "    \n",
    "    self.transformer = nn.ModuleDict(     # submodule indexing\n",
    "      dict(\n",
    "        wte = nn.Embedding(config.vocab_size, config.n_embd),               # weight token embd                  nn.Embedding -> wrapper for array of nums\n",
    "        wpe = nn.Embedding(config.block_size, config.n_embd),               # weight pos embd\n",
    "        h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),  # hidden         # index using numeric as per statedict\n",
    "        ln_f = nn.LayerNorm(config.n_embd),                                 # additional layernorm\n",
    "      ))\n",
    "    self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)  # classifier/head   n_embd -> vocab size\n",
    "  \n",
    "  def forward(self, idx, targets =None):\n",
    "    B,T = idx.size()\n",
    "    assert T <= self.config.block_size, f\"Block size {self.config.block_size} exceeds block size {self.config.block_size}\"\n",
    "    pos = torch.arange(0,T,dtype=torch.long,device=idx.device) #shape T\n",
    "    pos_emb = self.transformer.wpe(pos)                        # shape T,n_embd\n",
    "    tok_emb = self.transformer.wte(idx)                        # shape B,T,n_embd\n",
    "    x = tok_emb + pos_emb  # broadcast\n",
    "    \n",
    "    for block in self.transformer.h:\n",
    "      x = block(x)\n",
    "    x = self.transformer.ln_f(x)  # forward final layernorm\n",
    "    logits = self.lm_head(x)                                  # B,T,vocab_size\n",
    "    loss = None\n",
    "    if targets is not None:\n",
    "      loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))     # cross entropy doesnt prefer multidimensional inputs   logits = B*T,vocab\n",
    "    return logits,loss\n",
    "  \n",
    "  @classmethod\n",
    "  def from_pretrained(cls, model_type, override_args=None):  # load pretrained\n",
    "        assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n",
    "        override_args = override_args or {} # default to empty dict\n",
    "        # only dropout can be overridden see more notes below\n",
    "        from transformers import GPT2LMHeadModel\n",
    "        print(\"loading weights from pretrained gpt: %s\" % model_type)\n",
    "\n",
    "        # n_layer, n_head and n_embd are determined from model_type\n",
    "        config_args = {\n",
    "            'gpt2':         dict(n_layer=12, n_head=12, n_embd=768),  # 124M params\n",
    "            'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024), # 350M params\n",
    "            'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280), # 774M params\n",
    "            'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600), # 1558M params\n",
    "        }[model_type]\n",
    "        print(\"forcing vocab_size=50257, block_size=1024, bias=True\")\n",
    "        config_args['vocab_size'] = 50257 # always 50257 for GPT model checkpoints\n",
    "        config_args['block_size'] = 1024 # always 1024 for GPT model checkpoints\n",
    "        # create a from-scratch initialized minGPT model\n",
    "        config = GPTConfig()\n",
    "        model = GPT(config)\n",
    "        sd = model.state_dict()\n",
    "        sd_keys = sd.keys()\n",
    "        sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')] # discard this mask / buffer, not a param\n",
    "\n",
    "        # init a huggingface/transformers model\n",
    "        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
    "        sd_hf = model_hf.state_dict()\n",
    "\n",
    "        # copy while ensuring all the parameters are aligned and match in names and shapes\n",
    "        sd_keys_hf = sd_hf.keys()\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')] # ignore these, just a buffer\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')] # same, just the mask (buffer)\n",
    "        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
    "        # basically the openai checkpoints use a \"Conv1D\" module, but we only want to use a vanilla Linear\n",
    "        # this means that we have to transpose these weights when we import them\n",
    "        assert len(sd_keys_hf) == len(sd_keys), f\"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n",
    "        for k in sd_keys_hf:\n",
    "            if any(k.endswith(w) for w in transposed):\n",
    "                # special treatment for the Conv1D weights we need to transpose\n",
    "                assert sd_hf[k].shape[::-1] == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k].t())\n",
    "            else:\n",
    "                # vanilla copy over the other parameters\n",
    "                assert sd_hf[k].shape == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k])\n",
    "\n",
    "        return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e7d28adfde0f4f",
   "metadata": {},
   "source": [
    "Starting with Pretrained version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ace10f53ddd5fd55",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T12:37:42.268481Z",
     "start_time": "2024-06-20T12:37:40.110719Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading weights from pretrained gpt: gpt2\n",
      "forcing vocab_size=50257, block_size=1024, bias=True\n",
      "GPT(\n",
      "  (transformer): ModuleDict(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): CausalSelfAttention(\n",
      "          (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (attn_dropout): Dropout(p=0.2, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): MLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (gelu): GELU(approximate='none')\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = GPT.from_pretrained('gpt2')\n",
    "print(model) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b4394bea3c03db8c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T12:37:42.458698Z",
     "start_time": "2024-06-20T12:37:42.269565Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT(\n",
       "  (transformer): ModuleDict(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalSelfAttention(\n",
       "          (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.2, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (gelu): GELU(approximate='none')\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_return_sequences = 5\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "max_length = 30\n",
    "model.eval()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "907c3b17a28aa754",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T12:37:42.730299Z",
     "start_time": "2024-06-20T12:37:42.458698Z"
    }
   },
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "enc = tiktoken.get_encoding('gpt2')\n",
    "tokens = enc.encode(\"Hello World\")\n",
    "tokens =torch.tensor(tokens,dtype=torch.long,device=device)\n",
    "tokens = tokens.unsqueeze(0).repeat(num_return_sequences,1) #idx\n",
    "x = tokens.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3196e01a86f5cdf0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T12:37:43.226179Z",
     "start_time": "2024-06-20T12:37:42.730299Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Haroon\\AppData\\Local\\Temp\\ipykernel_13704\\218594534.py:50: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World, the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "Hello World, the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "Hello World, the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "Hello World, the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "Hello World, the the the the the the the the the the the the the the the the the the the the the the the the the the the\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "\n",
    "while x.size(1) < max_length:\n",
    "  with torch.no_grad():\n",
    "    logits = model(x)\n",
    "    logits = logits[:, -1, :] # take logits at end\n",
    "    probs = F.softmax(logits, dim=-1) \n",
    "    topx_probs, topx_tokens = torch.topk(probs, 50,dim=-1)  #huggingface default /keep top 50\n",
    "    ix = torch.multinomial(topx_probs,1)  # select from top\n",
    "    xcol = torch.gather(topx_tokens, -1, ix)  #gather corresponding index\n",
    "    x = torch.cat((x,xcol),dim=1) # append\n",
    "\n",
    "\n",
    "for i in range(num_return_sequences):\n",
    "  tokens = x[i,:max_length].tolist() #rows\n",
    "  decoded = enc.decode(tokens)\n",
    "  print(decoded)  # weird?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e2b2beb6532b8d",
   "metadata": {},
   "source": [
    "Now train from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d9797188285d14bf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T13:05:18.067989Z",
     "start_time": "2024-06-20T13:05:18.057944Z"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "B,T = 4,32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "27ed6674d46ffc08",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T13:05:18.695008Z",
     "start_time": "2024-06-20T13:05:18.612003Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2514, 9074, 13, 8858, 8270, 11, 4492, 13, 220, 198, 1273, 13]\n",
      "To Mrs. Savi\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "enc = tiktoken.get_encoding('gpt2')\n",
    "\n",
    "data = open('input.txt','r',encoding='utf-8').read()[:1000]   # start with only 1000 chars\n",
    "tokens = enc.encode(data)\n",
    "print(tokens[:12]); print(data[:12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "797fd2c3e0790c29",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T13:05:19.126463Z",
     "start_time": "2024-06-20T13:05:19.107733Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2514,  9074,    13,  8858,  8270,    11,  4492,    13,   220,   198,\n",
      "          1273,    13, 15722,  9228,    11,  4280,    13,  1367,   400,    11,\n",
      "          1596,    13,   198,  1639,   481, 46201,   284,  3285,   326,   645,\n",
      "          9336,   468],\n",
      "        [11791,   262,   198,  9503,   594,   434,   286,   281, 13953,   543,\n",
      "           345,   423, 11987,   351,   884,  6181,   198,   754,    65,   375,\n",
      "           654,    13,   314,  5284,   994,  7415,    11,   290,   616,   717,\n",
      "          4876,   318],\n",
      "        [  284, 19832,   198,  1820, 13674,  6621,   286,   616,  9490,   290,\n",
      "          3649,  6628,   287,   262,  1943,   198,  1659,   616, 25971,    13,\n",
      "           198,    40,   716,  1541,  1290,  5093,   286,  3576,    11,   290,\n",
      "           355,   314],\n",
      "        [ 2513,   287,   262,  6483,   286,   198,    47,  7307,  9228,    11,\n",
      "           314,  1254,   257,  4692,  7840, 28633,   711,  2402,   616, 25839,\n",
      "            11,   543,   198,  1671,  2114,   616, 25377,   290, 23816,   502,\n",
      "           351, 10974]], device='cuda:0')\n",
      "tensor([[ 9074,    13,  8858,  8270,    11,  4492,    13,   220,   198,  1273,\n",
      "            13, 15722,  9228,    11,  4280,    13,  1367,   400,    11,  1596,\n",
      "            13,   198,  1639,   481, 46201,   284,  3285,   326,   645,  9336,\n",
      "           468, 11791],\n",
      "        [  262,   198,  9503,   594,   434,   286,   281, 13953,   543,   345,\n",
      "           423, 11987,   351,   884,  6181,   198,   754,    65,   375,   654,\n",
      "            13,   314,  5284,   994,  7415,    11,   290,   616,   717,  4876,\n",
      "           318,   284],\n",
      "        [19832,   198,  1820, 13674,  6621,   286,   616,  9490,   290,  3649,\n",
      "          6628,   287,   262,  1943,   198,  1659,   616, 25971,    13,   198,\n",
      "            40,   716,  1541,  1290,  5093,   286,  3576,    11,   290,   355,\n",
      "           314,  2513],\n",
      "        [  287,   262,  6483,   286,   198,    47,  7307,  9228,    11,   314,\n",
      "          1254,   257,  4692,  7840, 28633,   711,  2402,   616, 25839,    11,\n",
      "           543,   198,  1671,  2114,   616, 25377,   290, 23816,   502,   351,\n",
      "         10974,    13]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# convert to shape B,T instead of single row\n",
    "import torch\n",
    "\n",
    "buf =  torch.tensor(tokens[:B*T + 1])  # take one extra for next token in y\n",
    "x = buf[:-1].view(B,T).to(device)   # exclude last\n",
    "y = buf[1:].view(B,T).to(device)     # start from second\n",
    "print(x); print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "153a1f1fa98de60c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T13:05:20.693005Z",
     "start_time": "2024-06-20T13:05:19.777990Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT(\n",
       "  (transformer): ModuleDict(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalSelfAttention(\n",
       "          (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.2, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (gelu): GELU(approximate='none')\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GPT(GPTConfig())\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b864978df1bfc626",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T13:06:14.836316Z",
     "start_time": "2024-06-20T13:06:14.595097Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(11.1579, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "logits, loss = model(x,y)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c52dfa7701c1a54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
